{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# Building classifiers for the Pump It Up project \n",
    "\n",
    "We did some initial data prep and eliminated a bunch of columns and did some data type conversions and filled in missing data. See the `data_prep.py` program for the details. In this notebook we will explore a few different classifier methods for this problem, including:\n",
    "\n",
    "* logistic regression with L1, L2 and elastic-net regularization\n",
    "* Random forest\n",
    "* Histogram-based gradient boosting classification tree\n",
    "* Ensemble models using the above models\n",
    "\n",
    "In order to use some of these models in sklearn, we will use some of [sklearn's preprocessing tools](https://scikit-learn.org/stable/modules/preprocessing.html#preprocessing) such as transformers, scaler, imputers and encoders.\n",
    "\n",
    "We will also use [Pipeline](https://scikit-learn.org/stable/modules/compose.html#pipeline) objects to chain together multiple modeling steps and streamline our modeling workflow.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# To auto-reload modules in jupyter notebook (so that changes in files *.py doesn't require manual reloading):\n",
    "# https://stackoverflow.com/questions/5364050/reloading-submodules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Import commonly used libraries and magic command for inline plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Load data from csv\n",
    "\n",
    "The `data_prep.py` program exported csv and json files. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "train_X_prep = pd.read_csv('./data/train_x.csv')\n",
    "test_X_prep = pd.read_csv(\"./data/test_x.csv\")\n",
    "train_y_prep = pd.read_csv(\"./data/train_y.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop id col\n",
    "train_X = train_X_prep.iloc[:, 1:]\n",
    "test_X = test_X_prep.iloc[:, 1:]\n",
    "train_y = train_y_prep.iloc[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing - variable type lists\n",
    "\n",
    "In order to use this data in sklearn, there is additional preprocessing that we will have to do. It will end up being useful to have a list of numeric columnss and categorical columns to facilitate preprocessing. The pandas `select_dtypes` method is useful for this - https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.select_dtypes.html.\n",
    "\n",
    "**NOTE:** Booleans in pandas show as object in `df.info()`. I converted the booleans to strings (ints are ok too) in `data_prep.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_cols = train_X.select_dtypes(include=['object']).columns.tolist()\n",
    "numeric_cols = train_X.select_dtypes(include=['number']).columns.tolist()\n",
    "\n",
    "all_cols = train_X.columns.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use an *assertion* to make sure we didn't miss any columns. The Python `assert` statement is a way to check that some condition, variable or expression has the desired and expected value. We use assertions quite a bit when we develop formal tests for our code as well as to catch important errors that might happen when our code is run. For example, we might use an assertion to make sure that some important calculation that should only return positive values, does so. The basic form of an assertion is:\n",
    "\n",
    "    assert some_boolean_expression [, some error message]\n",
    "    \n",
    "If the expression is True, nothing happens. But, if it's false an `AssertionError` is raised and Python will print out an error message. This is something we can trap. \n",
    "\n",
    "For our current situation, we believe we have classified all columns in our training data as being either categorical or numeric. One simple check would be that the number of entries in the `categorical_cols` list plus the number in the `numeric_cols` list should be equal to the number in the `all_cols` list. In other words..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(all_cols) == len(categorical_cols) + len(numeric_cols), 'each col should either be in categorical or numeric lists'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we run the cell above, nothing happens. That's great as it means the condition is true. \n",
    "\n",
    "Now let's show what happens if we make a mistake. I'll use new variable names for the demo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_cols_demo = train_X.select_dtypes(include=['object']).columns.tolist()\n",
    "# The next line is WRONG, should be include=['number']\n",
    "numeric_cols_demo = train_X.select_dtypes(include=['object']).columns.tolist() \n",
    "\n",
    "all_cols_demo = train_X.columns.tolist()\n",
    "\n",
    "# assert len(all_cols_demo) == len(categorical_cols_demo) + len(numeric_cols_demo), 'each col should either be in categorical or numeric lists'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learn more about assertions and defensive programming from the following Software Carpentry tutorial (part of the Python Programming lesson). You do need to be aware that assertions can be disabled and thus should not be used for critical data or security related validation tasks.\n",
    "\n",
    "https://swcarpentry.github.io/python-novice-inflammation/10-defensive/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Later we are also going to need the column index numbers for the variables that are categorical. Here's a little list comprehension to do that that uses the column name lists we just created. Note the use of the pandas `get_loc` method which returns the column index for a given column name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_cols_idx = [train_X.columns.get_loc(c) for c in categorical_cols]\n",
    "categorical_cols_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EASY CHALLENGE** Develop a similar list comprehension to get a list of column indexes for the numeric columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numeric_cols_idx =[WORK SOME MAGIC IN HERE]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This list of indices is helpful for things like the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X.iloc[:, categorical_cols_idx].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a logistic regression classifier\n",
    "\n",
    "Ok, here we go. There's a lot to digest in this section both with respect to the statistical method itself as well as to using sklearn. As a preview, we will be:\n",
    "\n",
    "* Reviewing the basics of logistic regression and learning about using *regularization* in our logistic regression models\n",
    "* Creating data transformers \n",
    "* Creating pipelines for transformations and model creation\n",
    "* Doing data partitioning, model fitting and model scoring\n",
    "* Gaining some intuition as to how various hyperparameters can affect the modeling process\n",
    "\n",
    "Some key resources include:\n",
    "\n",
    "* User Guide: https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
    "* API Docs: https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
    "\n",
    "Our problem is:\n",
    "\n",
    "* multiclass (not binary) with three classes for our target, the status of the well\n",
    "* currently, all of the categorical data are strings and sklearn doesn't like strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review of logistic regression basics\n",
    "\n",
    "If you need a refresher on the basics of logistic regression, I'm going to point you to two resources that cover it a level that is appropriate for this class.\n",
    "\n",
    "* Sections 4.1-4.3 in the [free ISLR book](https://statlearning.com/)\n",
    "* I've included the knitted HTML version of my logistic regression with R notes in the Downloads file. The filename is `IntroLogisticRegression_Loans_notes.html`. Also, here's the [link to the section of my pcda course website that covers logistic regression](http://www.sba.oakland.edu/faculty/isken/courses/mis5470_w21/modeling2_class_r.html).\n",
    "\n",
    "A few key points to remember about logistic regression:\n",
    "\n",
    "* We use it when we have a dependent (target) variable (i.e. the `y` variable) that takes on a fixed number of values. Frequently, our target variable is binary but we can use logistic regression for more than two output values (as in the Pump it Up problem).\n",
    "* Logistic regression fall within the family of regression models known as General Linear Models (in R, we use the `glm` function to fit these models)\n",
    "* We can interpret the predicted values from logistic regression models as probabilities and we can then use these probabilities to classify things.\n",
    "* Whereas in regular old multiple linear regression we can use least squares to find the best fit line (i.e. the values of the coefficients), in logistic regression we can't. Instead we need to use [maximum likelihood estimation](https://online.stat.psu.edu/stat415/lesson/1/1.2) (and [here for a nice video from the StatQuest guy](https://www.youtube.com/watch?v=XepXtl9YKwc)) and for that, various optimization methods are used to find the best values of the model coefficients. It's important to be aware of this since, as we'll see, we actually get to make choices regarding how sklearn solves this optimization problem. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularization methods for regression modeling\n",
    "\n",
    "It is not uncommon to have quite a few potential predictor variables when doing regression modeling. Often we might have significant multicollinearity and we can easily create overfitted models whose coefficients have high variance and that don't generalize well to unseen test data. One class of methods for dealing with this are known as *shrinkage methods* or *regularization methods*. It's a bit counterintuitive in that these methods try *shrink* the coefficient values towards zero in a controlled way. By doing this, we end up with a slightly biased model but one that has less variance. These methods can also act as a *variable selection* strategy since they can end up driving some coefficients to zero and essentially dropping them from the model. We won't go deeply into the math but you should have a basic understanding of how these methods work. It's particularly relevant when using logistic regression in sklearn since it does regularized logistic regression by default and there are some parameters you need to be able to intelligently set.\n",
    "\n",
    "The three main flavors of regularization that sklearn supports for logistic regression are:\n",
    "\n",
    "* Ridge regression\n",
    "* Lasso regression\n",
    "* Elastic-Net\n",
    "\n",
    "As we'll see below, each corresponds to a different *penalty function* that we use in the coefficient fitting process.\n",
    "\n",
    "I've marked up the [Logistic regression page from the sklearn User Guide](https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Image('images/regularization_overview.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the sklearn docs, they refer to $\\ell_1$ (lasso, and pronouced ell-one), $\\ell_2$ (ridge) and Elastic-Net regularization. You'll often see $\\ell_1$ and $\\ell_2$ written as $L1$ and $L2$ and they refer to quantities known as *norms* of a vector. You can think of them as different ways of measuring the length of a vector. $L2$ is related to Euclidean distance and, in terms of acting as a penalty factor in regression, is proportional to the sum of squares of the coefficients. In the PDSH book, JVP uses $\\theta$ to represent the coefficients and $\\alpha$ is a hyperparameter that control the relative weight put on the penalty term, $P$.\n",
    "\n",
    "$$\n",
    "P = \\alpha\\sum_{n=1}^N \\theta_n^2\n",
    "$$\n",
    "\n",
    "In the lasso case, the $L1$ norm is used for the penalty term and you can see it's just the sum of the absolute values of the coefficients (related to Manhattan distance):\n",
    "\n",
    "$$\n",
    "P = \\alpha\\sum_{n=1}^N |\\theta_n|\n",
    "$$\n",
    "\n",
    "While both of these serve the same basic purpose - shrink the coefficients towards zero, they affect the model fit somewhat differently. For some geometric reasons we won't get into, the lasso penalty will drive a number of coefficients to exactly zero. In essence, it acts a bit like a variable selection method since a zero valued coefficient is the same as dropping the variable from the model. There are pros and cons of each and both require setting the value of the hyperparameter $\\alpha$. In practice, cross-validation can be used to find a good value of $\\alpha$. You might have noticed that the sklearn `LogisticRegression` function does **not** have an $\\alpha$ parameter you can set. Instead you get to set $C$. Notice that all they've done is put the hyperparameter as a multiplier on the residual part of the fitting cost function. So,\n",
    "\n",
    "* higher values of $C$, means more weight on minimizing residuals and thus, less, regularization (low $\\alpha$).\n",
    "* lower values of $C$ means more weight on the regularization penalty term (high $\\alpha$).\n",
    "\n",
    "In other words, $C$ is an inverse of $\\alpha$. In sklearn, the default value of $C$ is 1. We'll play around with different $C$ values to see how they impact the model and later we'll use `LogisticRegressionCV` which will use cross-validation to find the best value of $C$ for our data.\n",
    "\n",
    "What about this Elastic-Net thing? Well, if we have two different penalty functions to choose from, why not let ourselves use them both? That's all Elastic-Net is doing. You get to set the value of the weight, $\\rho$ in the sklearn docs above, that is put on the $L1$ penalty and then $L2$ gets a weight of $1-\\rho$. \n",
    "\n",
    "**QUESTION** If $\\rho = 1$, is that equivalent to $L1$ or $L2$ regularization? And while we are at it, what would you set $C$ to if you didn't want any (or very little) regularization?\n",
    "\n",
    "**ADDITIONAL RESOURCES**\n",
    "\n",
    "* Section 6.2 in the ISLR book does a nice job in explaining regularization.\n",
    "* The PDSH notebook, 05.06-Linear-Regression.ipynb, has a good high level overview of regularization applied to regression. \n",
    "* There's a [nice Kaggle post on $L1$ vs $L2$ norms](https://www.kaggle.com/residentmario/l1-norms-versus-l2-norms) in the context of distance metrics and regularization. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a logistic regression pipeline\n",
    "\n",
    "In order to use a logistic regression classifier for our Pump it Up problem, we need to do a few steps in succession. At a high level, we need to do the following (many details to worry about in a bit):\n",
    "\n",
    "1. Do any required data transformations needed to use `LogisticRegression`.\n",
    "2. Create a `LogisticRegression` model object and set model options.\n",
    "3. Partition our data into training and test data.\n",
    "4. Fit the model.\n",
    "5. Score the model on training and test data.\n",
    "\n",
    "We'll take a closer look at each of these steps and then we will put them all together. Steps 1 and 2 will become `steps` in an sklearn `Pipeline` object. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1 - Data transformations\n",
    "\n",
    "Since we are doing regularized logistic regression, the numeric variables should be rescaled so that the units of measurement don't affect the model fitting process. The [sklearn preprocessing module](https://scikit-learn.org/stable/modules/preprocessing.html#) contains a bunch of utility functions for common tasks like this. We will use the [`StandardScalar`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html#sklearn.preprocessing.StandardScaler) which will transform numeric variables to have a mean of $0.0$ and a standard deviation of $1.0$. Yes, this puts values on a Z-scale. The `StandardScalar` is an example of a `Transformer` object and, much like we do with `Estimator` objects, we first instantiate a generic object and then later we'll actual use it on our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a StandardScalar object to use on our numeric variables\n",
    "numeric_transformer = StandardScaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the categorical variables, we need to do something to convert them from strings to numbers since `LogisticRegression` wants a numeric matrix for the predictor variables. If you remember your basic regression modeling, we need to create a set dummy binary variables for each categorical variable. For example, let's say we had a variable named `color` that took on the values 'yellow', 'blue' and 'green'. We could create three dummy variables that we might call, `color_yellow`, `color_blue`, and `color_green`.\n",
    "Here's a short example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_data_df = pd.DataFrame([{'color': 'blue', 'color_yellow': 0, 'color_blue': 1, 'color_green': 0}, \n",
    "                {'color': 'yellow', 'color_yellow': 1, 'color_blue': 0, 'color_green': 0},\n",
    "                {'color': 'blue', 'color_yellow': 0, 'color_blue': 1, 'color_green': 0},\n",
    "                {'color': 'green', 'color_yellow': 0, 'color_blue': 0, 'color_green': 1}])\n",
    "\n",
    "color_data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously, after creating the binary variables, the original `color` variable is dropped. You might recall from a basic statistics class that when we do this for a multiple linear regression model, we arbitrarily omit one of the binary variables. We do this for two reasons:\n",
    "\n",
    "* we can because if we know the value of any two out of the three binary variables, we know the value of the other one (since each row must have a single value of 1),\n",
    "* in least squares estimation we end up doing a matrix inversion and we don't want to have any variables that are perfect linear combinations of other variables as this can cause numeric instability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_data_df2 = pd.DataFrame([{'color': 'blue', 'color_yellow': 0, 'color_blue': 1}, \n",
    "                {'color': 'yellow', 'color_yellow': 1, 'color_blue': 0},\n",
    "                {'color': 'blue', 'color_yellow': 0, 'color_blue': 1},\n",
    "                {'color': 'green', 'color_yellow': 0, 'color_blue': 0}])\n",
    "\n",
    "color_data_df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**QUESTION** What is the linear equation for `color_green` as a function of `color_yellow` and `color_blue`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the machine learning world we often don't have to worry about this issue and will often just fully encode categorical variables with $0$'s and $1$'s. This is known as *one-hot encoding* and sklearn has a transformer object called `OneHotEncoder` to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_transformer = OneHotEncoder(handle_unknown='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we have created two transformers, one for our numeric feature variables and one for our categorical feature variables. Both need to be applied to our training and test data. To facilitate this, sklearn provides an estimator called a `ColumnTransformer`. We can use this to bundle multiple transformers together.\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.compose.ColumnTransformer.html\n",
    "\n",
    "Since these two transformers make up all of the things we need to do in our preprocessing step, I'm going to name my `ColumnTransformer` object accordingly. Look closely at what we are passing in to `ColumnTransformer()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_cols),\n",
    "        ('cat', categorical_transformer, categorical_cols)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are setting the `transformers` argument to a list of tuples. Each tuple contains:\n",
    "\n",
    "    (a name we pick for the transformer, the transformer object, which cols to apply this transformer to) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image('images/column_transformer.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>WAIT! What about the target variable? It's a string right now. Don't we have to encode it?</b>\n",
    "</div>\n",
    "\n",
    "This actually isn't answered prominently in the User Guide or API docs (afaik - I couldn't find it). But, a little searching led to this SO post in which it's confirmed that sklearn can handle strings for target variable values and it just calls `LabelEncoder` itself if needed to convert the strings to numbers. \n",
    "\n",
    "https://stackoverflow.com/questions/50201315/is-numerical-encoding-necessary-for-the-target-variable-in-classification\n",
    "\n",
    "Also notice the **remainder** parameter. As long as we are careful and don't include the `id` column in either of the variable lists (`numeric_cols` or `categorical_cols`), the transformer can handle automatically not including this variable in any model building. This is convenient since we eventually need the `id` column when creating submission files for the actual Pump it Up competition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2 - Create logistic regression model object\n",
    "\n",
    "Now that we've built the objects for doing preprocessing (just some data transformations in our case), we will create a logistic regression model object. Remember, this doesn't fit a model - we are just creating an \"empty\" model object. Well, it's not really empty as we can specify various optional parameter values that will impact the model fitting process down the line.\n",
    "\n",
    "Let's take a look at the API docs for `LogisticRegression` and discuss the options that we want to set.\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
    "\n",
    "In particular, let's focus on:\n",
    "\n",
    "* there's a lot of talk about *solvers* in the docs; which solver should we use?\n",
    "* we saw above that there are different regularization methods, each having a different penalty component of the cost function that is being minimized during model fitting (i.e. finding the best coefficient values). Which penalty should we use and how should we set the value of $C$?\n",
    "\n",
    "Since we are learning about regularization, we want to be able to try all three flavors - ridge, lasso, and Elastic-Net. Notice that the default values are:\n",
    "\n",
    "* `penalty='l2'` --> ridge\n",
    "* `C = 1`       --> balanced weights on residuals and regularization penalty\n",
    "* `solver='lbfgs'` --> it's the default because of its \"robustness\", according to the sklearn docs\n",
    "\n",
    "There's a nice table in the [User Guide section on logistic regression](https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression) that summarizes the capabilities of each solver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Image('images/penalties.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Learning to read API documentation is a valuable skill. It can often mean the difference between getting something to work and giving up.</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use the 'saga' solver since it's the only one that can do all three penalty types (ridge, lasso and Elastic-Net) and we want to experiment with all of them. Even though some of the values are the defaults, I'm going to explicitly specify them to make things more readable and so we don't have to remember what the defaults are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classifier model\n",
    "clf_model = LogisticRegression(penalty='l2', C=1, solver='saga', max_iter=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating a Pipeline for Steps 1 and 2\n",
    "[Pipelines](https://scikit-learn.org/stable/modules/compose.html#pipeline) let us chain estimators together. An example might be a sequence of transformations followed by a classifier. A `Pipeline` is made up of `steps`. I'm going to repeat the code we've developed so far and add on the `Pipeline`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create transformer objects\n",
    "numeric_transformer = StandardScaler()\n",
    "categorical_transformer = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "# Combine transformers into a preprocessor step\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_cols),\n",
    "        ('cat', categorical_transformer, categorical_cols)])\n",
    "\n",
    "# Classifier model\n",
    "clf_model = LogisticRegression(penalty='l2', C=1, solver='saga', max_iter=500)\n",
    "\n",
    "# Append classifier to preprocessing pipeline.\n",
    "# Now we have a full prediction pipeline.\n",
    "clf = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                      ('classifier', clf_model)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can even generate a little picture of your pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is from the new 1.0 ColumnTransformer example. \n",
    "from sklearn import set_config\n",
    "\n",
    "set_config(display='diagram')\n",
    "clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Steps 3 and 4 - Data partitioning and model fitting\n",
    "\n",
    "Since we only have target (`y`) data for our training dataset, we'll repartition `train_X` into new training (that we'll call `X_train` and `y_train`) and test datasets for purposes of model fitting and evaluation. For this we can use `sklearn.model_selection.train_test_split`. \n",
    "\n",
    "After partitioning the data, we'll fit the model with:\n",
    "\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "Notice that `clf` is actually our `Pipeline`. This illustrates an advantage of pipelines. We call `fit` one time on the pipeline and it handles the individual fits that happen with the transformers and then the classifier. Makes our code shorter and cleaner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Further partition our training data into train and test sets to use for model fitting and testing\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_X, train_y, test_size=0.2, random_state=14)\n",
    "\n",
    "# Fit model on new training data - notice that clf is actually the Pipeline\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Training score: {clf.score(X_train, y_train):.3f}\")\n",
    "print(f\"Test score: {clf.score(X_test, y_test):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we get a warning that coefficients aren't converging. First of all, what is it that's not converging? It's the *saga* solver. Under the hood, it's just trying to find coefficient values that optimizes the cost function associated with the penalty and the likelihood function. To do this it uses a type of optimization algorithm known as *stochastic gradient descent*. Depending on the details of a specific problem, such algorithms may or may not converge. Even if it doesn't converge the predictions might not be affected very much. For example, perhaps multicollinearity is causing weird oscillations in some coefficients. If it's not converging because it's just taking a long time to converge, we can increase the `max_iter` parameter to make it run longer. If you are interested in the details of stochastic gradient descent, I've included some info at the bottom of this notebook.\n",
    "\n",
    "Nevertheless, we are getting 0.750 accuracy level on the test data. Pretty unusual to have better performance on test than train but the difference is pretty small. This value is pretty stable no matter what I try for `max_iter`. Got same results with setting `C=1+e2` and `C=1+e1`. Let's try $C<1$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classifier model\n",
    "clf_model_C01 = LogisticRegression(penalty='l2', C=0.01, solver='saga', max_iter=500)\n",
    "\n",
    "# Append classifier to preprocessing pipeline.\n",
    "clf_C01 = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                      ('classifier', clf_model_C01)])\n",
    "\n",
    "# Fit model on training data \n",
    "clf_C01.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Training score: {clf_C01.score(X_train, y_train):.3f}\")\n",
    "print(f\"Test score: {clf_C01.score(X_test, y_test):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No convergence issues, but slightly worse performance. You might be thinking that we have quite a few degrees of freedom here with respect to the penalty function used, the hyperparameters ($C$ and for Elastic-Net, $\\rho$), the solver, and the number of iterations. Yep. And it can be quite difficult to know what to do in terms of making intelligent choices. That's why **the more you learn about how these statistical and ML algorithms work, the better you'll be in wielding them effectively**. For now, let's table this concern and do a bit more exploring.\n",
    "\n",
    "Let's try the lasso penalty with $C=0.001$. We are forcing a high level of regularization so that we can look at the coefficients to see if regularization is having an effect on them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classifier model\n",
    "clf_model_l1_C001 = LogisticRegression(penalty='l1', C=0.001, solver='saga', max_iter=500)\n",
    "\n",
    "# Append classifier to preprocessing pipeline.\n",
    "clf_l1_C001 = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                      ('classifier', clf_model_l1_C001)])\n",
    "\n",
    "# Fit model on training data \n",
    "clf_l1_C001.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Training score: {clf_l1_C001.score(X_train, y_train):.3f}\")\n",
    "print(f\"Test score: {clf_l1_C001.score(X_test, y_test):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not a big surprise, performance was worse - but that's not the point. Let's see how we can dig the fitted coefficients out of our models so that we can compare the coefficients for the model based on the $L2$ norm and high value of $C$ (low regularization) and the model we just ran with the $L1$ norm and a low value of $C$ (high regularization). \n",
    "\n",
    "First things first, how do we get at the coefficient values? The sklearn User Guide explains how to [access steps of a Pipeline](https://scikit-learn.org/stable/modules/compose.html#accessing-steps) and the following StackOverflow post also proved quite useful:\n",
    "\n",
    "* https://stackoverflow.com/questions/43856280/return-coefficients-from-pipeline-object-in-sklearn\n",
    "\n",
    "Since the coefficients are associated with one particular step in our pipeline, we have to use the `named_steps` attribute to get at them. Let's start just by getting their shape. I'll use our first model, the $L2$ norm with $C=1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.named_steps['classifier'].coef_.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why do we have three rows of coefficients? We have three possible outcomes for our target variable. For each outcome, we have a logistic regression model with 238 coefficients (!!!). Listing out 238 times 3 coefficents isn't going to be that useful. Let's plot them instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's adapt JVP's `basis_plot` function from the **05.06-Linear-Regression** notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_l1_C001.steps[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coef_plot(model, title=None):\n",
    "    fig, ax = plt.subplots(3, sharex=True)\n",
    "       \n",
    "    if title:\n",
    "        ax[0].set_title(title)\n",
    "\n",
    "    for i in range(3):\n",
    "        ax[i].plot(model.steps[1][1].coef_[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coef_plot(clf, 'L2, C=1.0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's plot the coefficients from the model that used the $L1$ norm and really low value of $C$ (high regularization). We are expecting a lot of the coefficients to get driven to 0. Does that happen?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coef_plot(clf_l1_C001, 'L1, C=0.001')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I also tried $C=1$ and performance was no better with $L1$ norm. Probably not likely that Elastic-Net will do any better since it's just a weighted average of $L1$ and $L2$ based penalties. \n",
    "\n",
    "Let's back up and see how sklearn can help us find good hyperparameter values, such as $C$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using cross-validation to find good hyperparameter values\n",
    "One nice feature of sklearn is that it builds in some useful tools like automating the process of using cross-validation to find optimal hyperparameter values. For logistic regression, there is a [LogisticRegressionCV](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegressionCV.html?highlight=logisticregressioncv#sklearn.linear_model.LogisticRegressionCV) function that will search over a grid of values for $C$ and find the best value. You just literally replace `LogisticRegression` with `LogisticRegressionCV` in the obvious place in our code. Unfortunately, I ran this and found that the optimal value is $C=10$ - suggesting that regularization is not super helpful for this problem.\n",
    "\n",
    "The sklearn [User Guide has a nice section on tuning hyperparameters](https://scikit-learn.org/stable/modules/grid_search.html#grid-search). Also see the [GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html?highlight=gridsearchcv#sklearn.model_selection.GridSearchCV) function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making predictions on the contest test data\n",
    "\n",
    "Even though we have a model with under 80% accuracy, let's make predictions on the real Pump it Up contest `test_X` data.\n",
    "\n",
    "Notice how easy the use of pipelines make this. We simply refit a model on the entire training data set and then can use the `predict` method for the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final logistic regression classifier model\n",
    "clf_LR_model_final = LogisticRegression(penalty='l2', C=10, solver='saga', max_iter=500)\n",
    "\n",
    "# Append classifier to preprocessing pipeline.\n",
    "clf_LR_final = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                      ('classifier', clf_LR_model_final)])\n",
    "\n",
    "# Fit model on training data \n",
    "clf_LR_final.fit(train_X, train_y)\n",
    "print(\"Training score: %.3f\" % clf_LR_final.score(train_X, train_y))\n",
    "\n",
    "# Make predictions on the test data\n",
    "clf_LR_final_predictions = clf_LR_final.predict(test_X)\n",
    "print(clf_LR_final_predictions[:10])  # Print out a few predictions just to see what they look like\n",
    "\n",
    "# Need to bring back the id field to create a submission file for Pump it Up competition\n",
    "submit_dict = {'id': test_X_prep['id'],\n",
    "              'status_group': clf_LR_final_predictions}\n",
    "\n",
    "clf_LR_final_submission = pd.DataFrame(submit_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_LR_final_submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_LR_final_submission.to_csv('./output/clf_LR_final_submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not surprisingly, the performance of this model on the real test data is not terrific."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image('images/LR_C10_score.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further automation and model persistence\n",
    "\n",
    "I decided I wanted to try to fit a whole bunch of logistic regression models corresponding to different hyperparameter combinations. In particular, I did separate `LogisticRegressionCV` fits for each of the three penalties: l1, l2, elasticnet. Taking the default C leads to grid in powers of 10 [1e-1, 1e+4]. for ElasticNet we can also search over the l1_ratio [0, 1]. See the script, `logistic.py` if you are interested. \n",
    "\n",
    "This took a while to run and when it was done, I dumped a huge dictionary of fitted models to a [Python pickle file](https://docs.python.org/3/library/pickle.html), `output/models_cls.pkl`. In other words, I wanted a way to do *model persistence* - the ability to reuse the models without retraining them. [Serializing](https://en.wikipedia.org/wiki/Serialization) the models to a pickle file is just one way to do this. Specialized markup languages have been devised for representing model objects and making them interoperable across computing platforms. Two such languages are:\n",
    "\n",
    "* [Predictive Modeling Markup Language (PMML)](http://dmg.org/pmml/v4-4-1/GeneralStructure.html) - XML based, so human and machine readable\n",
    "* [Open Neural Network Exchange (ONNX)](https://onnx.ai/) - a binary serialization format\n",
    "\n",
    "There are tools available for converting sklearn models to these formats. See the sklearn [Model Persistence section](https://scikit-learn.org/stable/modules/model_persistence.html) of the User Guide."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll just do a short demo of loading the pickled models and accessing a few model object attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./output/models_clf.pkl\", \"rb\") as model_file:\n",
    "    models_clf = pickle.load(model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(models_clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_clf.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_l1 = models_clf['l1']\n",
    "model_l2 = models_clf['l2']\n",
    "model_elasticnet = models_clf['elasticnet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(model_l1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dir(model_l1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# All the public model attributes that we might ever want to look at or use\n",
    "[att for att in dir(model_l1) if not att.startswith('_')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get target classes\n",
    "model_l1.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See details of the pipeline\n",
    "model_l1.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to go into `steps` to see things like cofficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# steps is a list of step tuples\n",
    "model_l1.steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This should be label of first step (zero index)\n",
    "model_l1.steps[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[att for att in dir(model_l1.steps[1][1]) if not att.startswith('_')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_elasticnet.steps[1][1].l1_ratio_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From above we see that an `l1_ratio_` of 1 in elastic-net means it's doing l1 regularization (lasso) for each outcome. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>BOTTOM LINE 1:</b> You can save models and reuse them without retraining. Obviously, this is a useful first step towards deploying models in practice.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>BOTTOM LINE 2:</b> Often, extensive searching over a grid of hyperparameter values doesn't lead to much model performance gains. Getting the hyperparameters set reasonably well is often sufficient. The real gains usually come through feature engineering. How can you use your domain knowledge to create features that end up with high predictive power? This is the art of modeling. Anyone can throw a bunch of data into some ML pipeline. It's knowing which data to throw in that is the real superpower.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beyond logistic regression\n",
    "\n",
    "Let's try some more techniques and get more practice with sklearn pipelines. Then we could bake them all together into an ensemble which would make a nice wrap up of this module. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random forest\n",
    "\n",
    "Unfortunately, the sklearn implementation of random forests does **not** handle categorical data in string format (as we could do in R with `randomForest`). The only valid way to encode our categorical variables is with one-hot encoding. This can lead to huge trees requiring many splits. Let's give it a whirl. \n",
    "\n",
    "Notice how simple it is to just swap in a new model technique into our pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I'm repeating the transformer here but not needed\n",
    "numeric_transformer = StandardScaler()\n",
    "categorical_transformer = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_cols),\n",
    "        ('cat', categorical_transformer, categorical_cols)])\n",
    "\n",
    "# Append random forest classifier to preprocessing pipeline.\n",
    "clf_rf = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                      ('classifier', RandomForestClassifier(oob_score=True, random_state=0))])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_X, train_y, test_size=0.2,\n",
    "                                                    random_state=0)\n",
    "\n",
    "clf_rf.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Training score: {clf_rf.score(X_train, y_train):.3f}\")\n",
    "print(f\"Test score: {clf_rf.score(X_test, y_test):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whoa, pretty high accuracy on the training data and a pretty big drop in the test data accuracy. Even so, looks like it will outperform the logistic regression model.\n",
    "\n",
    "**QUESTION** Is this evidence of overfitting or underfitting?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the confusion matrices for both the training and test data.\n",
    "\n",
    "https://scikit-learn.org/stable/visualizations.html#visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import plot_confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do the training data first. Given the near perfect accuracy score, you should already have a good idea what this matrix will look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles_options = [(\"Confusion matrix for train, without normalization\", None),\n",
    "                  (\"Normalized confusion matrix for train\", 'true')]\n",
    "\n",
    "class_names = clf_rf['classifier'].classes_\n",
    "\n",
    "for title, normalize in titles_options:\n",
    "    disp = plot_confusion_matrix(clf_rf, X_train, y_train,\n",
    "                                 display_labels=class_names,\n",
    "                                 cmap=plt.cm.Blues,\n",
    "                                 normalize=normalize)\n",
    "    disp.ax_.set_title(title)\n",
    "\n",
    "    print(title)\n",
    "    print(disp.confusion_matrix)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now do the same for the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles_options = [(\"Confusion matrix for test, without normalization\", None),\n",
    "                  (\"Normalized confusion matrix for test\", 'true')]\n",
    "\n",
    "class_names = clf_rf['classifier'].classes_\n",
    "\n",
    "for title, normalize in titles_options:\n",
    "    disp = plot_confusion_matrix(clf_rf, X_test, y_test,\n",
    "                                 display_labels=class_names,\n",
    "                                 cmap=plt.cm.Blues,\n",
    "                                 normalize=normalize)\n",
    "    disp.ax_.set_title(title)\n",
    "\n",
    "    print(title)\n",
    "    print(disp.confusion_matrix)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not surprisingly, 'functional needs repair' seems toughest to predict."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's refit on the entire training data set and make predictions on the real contest test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final random forest classifier model\n",
    "clf_RF_model_final = RandomForestClassifier(oob_score=True, random_state=0)\n",
    "\n",
    "# Append classifier to preprocessing pipeline.\n",
    "clf_RF_final = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                      ('classifier', clf_RF_model_final)])\n",
    "\n",
    "# Fit model on training data \n",
    "clf_RF_final.fit(train_X, train_y)\n",
    "print(\"Training score: %.3f\" % clf_RF_final.score(train_X, train_y))\n",
    "\n",
    "# Make predictions on the test data\n",
    "clf_RF_final_predictions = clf_RF_final.predict(test_X)\n",
    "print(clf_RF_final_predictions[:10])  # Print out a few predictions just to see what they look like\n",
    "\n",
    "# Create submission file \n",
    "submit_dict = {'id': test_X_prep['id'],\n",
    "              'status_group': clf_RF_final_predictions}\n",
    "\n",
    "clf_RF_submission = pd.DataFrame(submit_dict, columns=['id', 'status_group'])\n",
    "clf_RF_submission.to_csv('./output/clf_RF_submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This submission got a score of 0.8101 which was a big improvement over the logistic regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm going to save our final logistic regression and random forest models (actually pipelines) in a pickle file. I might decide to want to use these in some other notebook (yes, I use them in the optional gradient boosting notebook)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_clf_rf = {'clf_LR': clf_LR_final,\n",
    "                'clf_RF': clf_RF_final}\n",
    "\n",
    "with open(\"output/models_clf_rf.pkl\", \"wb\") as model_file:\n",
    "    pickle.dump(models_clf_rf, model_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning More\n",
    "\n",
    "Start with: https://xkcd.com/1838/ and then get it explained a little more by https://www.explainxkcd.com/wiki/index.php/1838:_Machine_Learning. :)\n",
    "\n",
    "Josh Starmer, the [StatQuest guy](https://www.youtube.com/channel/UCtYLUTtgS3k1Fg4y5tAhLbw), has a nice video tutorial on [gradient descent](https://www.youtube.com/watch?v=sDv4f4s2SB8) **BAM!**. Right at the end of this video, he gives a very concise explanation of stochastic gradient descent - just use a random sample of the dataset when computing the gradient to speed things up. He follows up with another video tutorial dedicated to explaining [stochastic gradient descent](https://www.youtube.com/watch?v=vMh0zPT0tLI). **DOUBLE BAM!!**\n",
    "\n",
    "I'm also a fan of Joel Grus's [Data Science from Scratch](https://www.oreilly.com/library/view/data-science-from/9781492041122/) book (2ed just came out earlier this year). It's all Python and it's a great approach to learning the central ideas behind how various data science algorithms work. He's got a chapter on gradient descient - get the code at his GitHub site for the book: https://github.com/joelgrus/data-science-from-scratch\n",
    "\n",
    "### Misc additional sklearn related docs I used while putting this together\n",
    "\n",
    "**Pipelines** \n",
    "\n",
    "* https://scikit-learn.org/stable/modules/compose.html#combining-estimators\n",
    "* https://scikit-learn.org/stable/modules/compose.html#\n",
    "https://stackoverflow.com/questions/28822756/getting-model-attributes-from-scikit-learn-pipeline\n",
    "\n",
    "**Column transformers**\n",
    "\n",
    "* https://scikit-learn.org/stable/modules/generated/sklearn.compose.ColumnTransformer.html#sklearn.compose.ColumnTransformer\n",
    "* https://scikit-learn.org/stable/modules/generated/sklearn.compose.make_column_transformer.html#sklearn.compose.make_column_transformer\n",
    "* https://scikit-learn.org/dev/auto_examples/compose/plot_column_transformer_mixed_types.html#use-columntransformer-by-selecting-column-by-names\n",
    "* https://scikit-learn.org/stable/auto_examples/compose/plot_column_transformer.html#sphx-glr-auto-examples-compose-plot-column-transformer-py\n",
    "\n",
    "**Logistic regression solvers in sklearn**\n",
    "\n",
    "* Detailed SO post on logistic regression solvers: https://stackoverflow.com/questions/38640109/logistic-regression-python-solvers-defintions/52388406#52388406\n",
    "* https://towardsdatascience.com/dont-sweat-the-solver-stuff-aea7cddc3451\n",
    "* https://stackoverflow.com/questions/45850841/does-sklearn-linear-model-logisticregression-always-converge-to-best-solution\n",
    "* https://datascience.stackexchange.com/questions/77813/logistic-regression-does-cannot-converge-without-poor-model-performance\n",
    "\n",
    "**open data source**\n",
    "\n",
    "openml: https://scikit-learn.org/stable/datasets/loading_other_datasets.html#openml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answers to questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_cols_idx = [train_X.columns.get_loc(c) for c in numeric_cols]\n",
    "numeric_cols_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**QUESTION** If $\\rho = 1$, is that equivalent to $L1$ or $L2$ regularization? And while we are at it, what would you set $C$ to if you didn't want any (or very little) regularization?\n",
    "\n",
    "**ANSWER**\n",
    "\n",
    "If $\\rho = 1$, elastic-net becomes $L1$ (lasso) regularization. If you want little or no regularization, set $C$ to a high number as this puts heavy weight on the \"residual\" part of the loss function used in fitting the coefficients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**QUESTION** What is the linear equation for `color_green` as a function of `color_yellow` and  `color_blue`?\n",
    "\n",
    "**ANSWER**\n",
    "\n",
    "`color_green` = 1 - `color_yellow` - `color_blue`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**QUESTION** Is this evidence of overfitting or underfitting?\n",
    "\n",
    "**ANSWER** Super high accuracy on training data followed by a plunge in accuracy on test data is a sign of overfitting. Your model as \"learned\" the training data but generalizes poorly to new data. It's like fitting a high order polynomial to a scatter plot. It may fit the points well but will do terrible on new data since the actual relationship is probably much simpler (e.g. quadratic or cubic or logarithmic or exponential)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
